{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25da37b-21c7-4515-bcbd-1516c7903925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6543da4-2218-4412-a899-a9186b1e84ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading datasets\n",
    "train_df = pd.read_csv('train_final.csv')\n",
    "test_df = pd.read_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09016b21-2057-4d86-a77a-32f46d739775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for '?' and NaN values in each column for both train and test sets\n",
    "def check_missing_values(df, name):\n",
    "    print(f\"\\n--- {name} Data ---\")\n",
    "    nan_counts = df.isnull().sum()  # Counts of NaN values\n",
    "    question_mark_counts = (df == '?').sum()  # Counts of '?' values\n",
    "\n",
    "    # Combine the counts into a single DataFrame for easy comparison\n",
    "    missing_value_summary = pd.DataFrame({\n",
    "        'NaN Count': nan_counts,\n",
    "        '? Count': question_mark_counts\n",
    "    })\n",
    "\n",
    "    print(missing_value_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2adb5ec4-9c81-4193-9698-728d51623b74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Train Data ---\n",
      "                NaN Count  ? Count\n",
      "age                     0        0\n",
      "workclass               0     1437\n",
      "fnlwgt                  0        0\n",
      "education               0        0\n",
      "education.num           0        0\n",
      "marital.status          0        0\n",
      "occupation              0     1442\n",
      "relationship            0        0\n",
      "race                    0        0\n",
      "sex                     0        0\n",
      "capital.gain            0        0\n",
      "capital.loss            0        0\n",
      "hours.per.week          0        0\n",
      "native.country          0      427\n",
      "income>50K              0        0\n",
      "\n",
      "--- Test Data ---\n",
      "                NaN Count  ? Count\n",
      "ID                      0        0\n",
      "age                     0        0\n",
      "workclass               0     1362\n",
      "fnlwgt                  0        0\n",
      "education               0        0\n",
      "education.num           0        0\n",
      "marital.status          0        0\n",
      "occupation              0     1367\n",
      "relationship            0        0\n",
      "race                    0        0\n",
      "sex                     0        0\n",
      "capital.gain            0        0\n",
      "capital.loss            0        0\n",
      "hours.per.week          0        0\n",
      "native.country          0      430\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in train and test data - only ? should be there\n",
    "check_missing_values(train_df, 'Train')\n",
    "check_missing_values(test_df, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1617188-8aa2-4863-a580-44a042ed8996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace '?' with NaN to handle missing values\n",
    "train_df.replace('?', pd.NA, inplace=True)\n",
    "test_df.replace('?', pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6630a54-a679-433e-b3d2-03f64e68e422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Train Data ---\n",
      "                NaN Count  ? Count\n",
      "age                     0        0\n",
      "workclass            1437        0\n",
      "fnlwgt                  0        0\n",
      "education               0        0\n",
      "education.num           0        0\n",
      "marital.status          0        0\n",
      "occupation           1442        0\n",
      "relationship            0        0\n",
      "race                    0        0\n",
      "sex                     0        0\n",
      "capital.gain            0        0\n",
      "capital.loss            0        0\n",
      "hours.per.week          0        0\n",
      "native.country        427        0\n",
      "income>50K              0        0\n",
      "\n",
      "--- Test Data ---\n",
      "                NaN Count  ? Count\n",
      "ID                      0        0\n",
      "age                     0        0\n",
      "workclass            1362        0\n",
      "fnlwgt                  0        0\n",
      "education               0        0\n",
      "education.num           0        0\n",
      "marital.status          0        0\n",
      "occupation           1367        0\n",
      "relationship            0        0\n",
      "race                    0        0\n",
      "sex                     0        0\n",
      "capital.gain            0        0\n",
      "capital.loss            0        0\n",
      "hours.per.week          0        0\n",
      "native.country        430        0\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in train and test data - only nan should be there\n",
    "check_missing_values(train_df, 'Train')\n",
    "check_missing_values(test_df, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bccae917-ade6-4bde-82cf-e80396c1d4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate target variable and features\n",
    "X_train = train_df.drop(columns='income>50K')  \n",
    "y_train = train_df['income>50K']              \n",
    "X_test = test_df.copy()                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b47064aa-d23b-4c51-b710-8a46299937a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# categorical and numerical (i.e. continous) columns\n",
    "numerical_features = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\n",
    "categorical_features = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bc73780-f544-48cf-a7d6-399b44bb219c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# handling numerical columns\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4f12521-f127-42ca-8477-4413ea2a0152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# handling categorical columns\n",
    "\n",
    "# Ensure all categorical columns, including 'sex', are treated as strings\n",
    "X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "X_test[categorical_features] = X_test[categorical_features].astype(str)\n",
    "\n",
    "# Count the unique values in each categorical column\n",
    "unique_category_counts = X_train[categorical_features].nunique()\n",
    "\n",
    "# Set a threshold for low vs. high cardinality\n",
    "threshold = 15\n",
    "low_cardinality_cols = unique_category_counts[unique_category_counts <= threshold].index.tolist()\n",
    "high_cardinality_cols = unique_category_counts[unique_category_counts > threshold].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eeb58bc-15e4-43ea-a6d3-faa8c4c64436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['workclass', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\n",
      "['education', 'native.country']\n"
     ]
    }
   ],
   "source": [
    "print(low_cardinality_cols)\n",
    "print(high_cardinality_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d89bf64-c8cb-4511-b95d-c43a1bc5e838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handle low cardinality categorical features (OneHotEncoder)\n",
    "#for now both are one-hot encoded let's work on it by uncommenting the ordinal code\n",
    "categorical_transformer_low = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "categorical_transformer_high = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))  # Handle unknown categories\n",
    "])\n",
    "\n",
    "# Combine the preprocessing steps into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat_low', categorical_transformer_low, low_cardinality_cols),\n",
    "        ('cat_high', categorical_transformer_high, high_cardinality_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the preprocessing pipeline to the data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39d202c7-0de8-4311-9122-953091f238c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC: 0.9075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leelasowmya/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "logreg = LogisticRegression(random_state=0)\n",
    "\n",
    "# Training Logistic Regression\n",
    "logreg.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predicting\n",
    "y_train_pred_logreg = logreg.predict_proba(X_train_transformed)[:, 1]  # Probability for class 1 (income > 50K)\n",
    "\n",
    "# Evaluate Logistic Regression Model using Area Under ROC (AUC) curve\n",
    "logreg_auc = roc_auc_score(y_train, y_train_pred_logreg)\n",
    "print(f\"Logistic Regression AUC: {logreg_auc:.4f}\")\n",
    "\n",
    "# Generate predictions on the test data using Logistic Regression Model\n",
    "y_test_pred_logreg = logreg.predict_proba(X_test_transformed)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f2fdc80-6bb3-4a7a-84cb-c817ae4d9ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression submission saved as submission_logreg.csv\n"
     ]
    }
   ],
   "source": [
    "if 'Id' in test_df.columns:\n",
    "    test_ids = test_df['Id']\n",
    "else:\n",
    "    test_ids = test_df.index + 1 \n",
    "\n",
    "# Create submission dataframe for Logistic Regression\n",
    "submission_logreg = pd.DataFrame({\n",
    "    'ID': test_ids,  # Use 'Id' from the test set or adjusted index if missing\n",
    "    'Prediction': y_test_pred_logreg\n",
    "})\n",
    "\n",
    "submission_file_name = 'submission_logreg.csv'\n",
    "\n",
    "# Save predictions to CSV for submission in the correct format\n",
    "submission_logreg.to_csv(submission_file_name, index=False)\n",
    "print(f\"Logistic Regression submission saved as {submission_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c90bb6e-0adb-4e41-9068-ba77588f6a46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree AUC: 0.9273\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Model \n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=10, min_samples_leaf=5)\n",
    "\n",
    "# Train Decision Tree\n",
    "decision_tree.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predicting\n",
    "y_train_pred_tree = decision_tree.predict_proba(X_train_transformed)[:, 1] \n",
    "\n",
    "# Evaluate Decision Tree Model using Area Under ROC (AUC) curve\n",
    "tree_auc = roc_auc_score(y_train, y_train_pred_tree)\n",
    "print(f\"Decision Tree AUC: {tree_auc:.4f}\")\n",
    "\n",
    "# Generate predictions on the test data using Decision Tree Model\n",
    "y_test_pred_tree = decision_tree.predict_proba(X_test_transformed)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81dae24c-e086-4d21-ad23-ef5ebe15bbff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree submission saved as submission_tree.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'Id' column exists in test set for submission\n",
    "if 'Id' in test_df.columns:\n",
    "    test_ids = test_df['Id']\n",
    "else:\n",
    "    test_ids = test_df.index + 1  \n",
    "    \n",
    "# Create submission dataframe for Decision Tree\n",
    "submission_tree = pd.DataFrame({\n",
    "    'ID': test_ids,  # Use 'Id' from the test set or adjusted index if missing\n",
    "    'Prediction': y_test_pred_tree\n",
    "})\n",
    "\n",
    "submission_file_name = 'submission_tree.csv'\n",
    "\n",
    "# Save predictions to CSV for submission in the correct format\n",
    "submission_tree.to_csv(submission_file_name, index=False)\n",
    "\n",
    "print(f\"Decision Tree submission saved as {submission_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f28f5-ec73-42ce-ad52-78208d751180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
